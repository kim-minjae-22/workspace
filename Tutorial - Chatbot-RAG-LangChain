{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install -q langchain langchain-community langchain_huggingface chromadb wikipedia","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T05:01:24.068744Z","iopub.execute_input":"2024-11-05T05:01:24.069170Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\njupyterlab 4.2.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires kubernetes<27,>=8.0.0, but you have kubernetes 31.0.0 which is incompatible.\nkfp 2.5.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nthinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import os\nfrom langchain_community.document_loaders import WikipediaLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_community.vectorstores import Chroma\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T05:02:32.938936Z","iopub.execute_input":"2024-11-05T05:02:32.939400Z","iopub.status.idle":"2024-11-05T05:02:34.594715Z","shell.execute_reply.started":"2024-11-05T05:02:32.939353Z","shell.execute_reply":"2024-11-05T05:02:34.593575Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"YOUR-API-TOKEN\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T05:02:43.211206Z","iopub.execute_input":"2024-11-05T05:02:43.211787Z","iopub.status.idle":"2024-11-05T05:02:43.217529Z","shell.execute_reply.started":"2024-11-05T05:02:43.211740Z","shell.execute_reply":"2024-11-05T05:02:43.216392Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T05:02:43.974908Z","iopub.execute_input":"2024-11-05T05:02:43.975340Z","iopub.status.idle":"2024-11-05T05:02:43.983685Z","shell.execute_reply.started":"2024-11-05T05:02:43.975295Z","shell.execute_reply":"2024-11-05T05:02:43.982505Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'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'"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"from langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.llms import HuggingFaceHub\n\n# set Korean embedding and llm odel\nhf_embeddings = HuggingFaceEmbeddings(model_name=\"jhgan/ko-sroberta-multitask\")\n\nhf_llm = HuggingFaceHub(\n    repo_id=\"skt/kogpt2-base-v2\",\n    model_kwargs={\"task\": \"text-generation\"}\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T05:02:45.037930Z","iopub.execute_input":"2024-11-05T05:02:45.038912Z","iopub.status.idle":"2024-11-05T05:03:16.806546Z","shell.execute_reply.started":"2024-11-05T05:02:45.038853Z","shell.execute_reply":"2024-11-05T05:03:16.805212Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/3726747796.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n  hf_embeddings = HuggingFaceEmbeddings(model_name=\"jhgan/ko-sroberta-multitask\")\n/opt/conda/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm, trange\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cdd641bb6fa4d9286381d90b0a704ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/123 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d41a63eb9ba24d818b12c40bf3c19d31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.86k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd46069c5f964f14ada4cf6a8818baa7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4677d6a983f64da48147f2d3c6d68f7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/744 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dfb2d06599c403eb4d3b8a1127a306a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc8c91f6a9264936817aa241d1c1147d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/585 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd812a3d1ad549f58910fa72aea44a6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/248k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c644d39cbaa4bdcbd300168192efc69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/495k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fdc256e943543e8a283c7267afc1b1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/156 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5edabda29e9e45e2a45025ab7415eb14"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"806eeade984f43e19d5ed36f0eeb5636"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_31/3726747796.py:7: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n  hf_llm = HuggingFaceHub(\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import requests\nfrom langchain.schema import Document\nfrom bs4 import BeautifulSoup\n\n# By default, English documents (https://en.wikipedia.org))\n# def load_Wiki_docs(\"흑백요리사\"):\n#     loader = WikipediaLoader(query=query, load_max_docs=1)\n#     documents = loader.load()\n    \n#     text_splitter = RecursiveCharacterTextSplitter(\n#         chunk_size=1000,\n#         chunk_overlap=200\n#     )\n#     splits = text_splitter.split_documents(documents)\n    \n#     return splits\n\n\n# For Korean query, get results from Korean wikipedia website and crawl and parse results\ndef load_Korean_wiki_docs(topic):\n\n    url = f\"https://ko.wikipedia.org/wiki/{topic}\"\n    \n    response = requests.get(url)\n    response.raise_for_status()  # raise Exception when error occurs\n\n    # HTML parsing and extract body contents\n    soup = BeautifulSoup(response.text, 'html.parser')\n    content = soup.find('div', {'class': 'mw-parser-output'})  # find div including body contents \n    \n    # Extract contents\n    paragraphs = content.find_all('p')\n    text = \"\\n\".join([p.get_text() for p in paragraphs])  # concat all context in <p> tags \n \n    # convert to Document object (required for LangChain)\n    documents = [Document(page_content=text, metadata={\"source\": url})]\n    \n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,\n        chunk_overlap=200\n    )\n    splits = text_splitter.split_documents(documents)\n    \n    return splits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T05:03:16.809052Z","iopub.execute_input":"2024-11-05T05:03:16.809841Z","iopub.status.idle":"2024-11-05T05:03:17.030325Z","shell.execute_reply.started":"2024-11-05T05:03:16.809796Z","shell.execute_reply":"2024-11-05T05:03:17.029304Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def create_vectorstore(splits): \n    vectorstore = Chroma.from_documents(documents=splits, embedding=hf_embeddings)\n    return vectorstore","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T05:03:17.031947Z","iopub.execute_input":"2024-11-05T05:03:17.033324Z","iopub.status.idle":"2024-11-05T05:03:17.039953Z","shell.execute_reply.started":"2024-11-05T05:03:17.033266Z","shell.execute_reply":"2024-11-05T05:03:17.038722Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"topic = \"흑백요리사\"\n# Load wikipedia documents for this topic\nsplits = load_Korean_wiki_docs(topic)\n# Create vectorstore with this fetched docs\nvectorstore = create_vectorstore(splits)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T05:03:17.043019Z","iopub.execute_input":"2024-11-05T05:03:17.044256Z","iopub.status.idle":"2024-11-05T05:03:18.565950Z","shell.execute_reply.started":"2024-11-05T05:03:17.044193Z","shell.execute_reply":"2024-11-05T05:03:18.564772Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"splits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T05:04:05.539350Z","iopub.execute_input":"2024-11-05T05:04:05.539829Z","iopub.status.idle":"2024-11-05T05:04:05.548469Z","shell.execute_reply.started":"2024-11-05T05:04:05.539788Z","shell.execute_reply":"2024-11-05T05:04:05.546996Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"[Document(metadata={'source': 'https://ko.wikipedia.org/wiki/흑백요리사'}, page_content='《흑백요리사: 요리 계급 전쟁》(영어: Culinary Class Wars)은 넷플릭스의 요리 서바이벌 프로그램이다. 방송 직후 세계 여러 나라에서 시청률 1위를 기록했고, 대만인들의 한국 관광 열풍과 한국 음식에 대한 사랑을 불러일으켰다. 유명 레스토랑 셰프 등 100인의 요리사가 출연한다. 심사위원은 백종원과 안성재가 맡았다. 가제는 《호날두요리사》였다.[1]')]"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"## 1. Use langchain RAG","metadata":{}},{"cell_type":"code","source":"def create_rag_chain(vectorstore):\n    prompt_template = \"\"\"문맥을 참고하여 질문에 정확하고 간결하게 답하십시오.\n    문맥: {context}\n    질문: {question}\n    답변:\"\"\"\n    PROMPT = PromptTemplate(\n        template=prompt_template, input_variables=[\"context\", \"question\"]\n    )\n\n    chain_type_kwargs = {\"prompt\": PROMPT}\n\n    # Make context shorter\n    # def short_context(context, max_length=300):\n    #     return context[:max_length] if len(context) > max_length else context\n    \n    # class ShortContextRetriever(BaseRetriever):\n    #     def __init__(self, retriever):\n    #         super().__init__()\n    #         self._retriever = retriever\n        \n    #     def get_relevant_documents(self, query):\n    #         docs = self._retriever.get_relevant_documents(query)\n    #         for doc in docs:\n    #             doc.page_content = short_context(doc.page_content)\n    #         return docs\n    \n    # retriever = ShortContextRetriever(vectorstore.as_retriever())\n    \n    qa_chain = RetrievalQA.from_chain_type(\n        llm=hf_llm,\n        chain_type=\"stuff\",\n        retriever=vectorstore.as_retriever(),\n        chain_type_kwargs=chain_type_kwargs,\n        return_source_documents=True\n    )\n    \n    return qa_chain","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T05:04:10.446591Z","iopub.execute_input":"2024-11-05T05:04:10.447027Z","iopub.status.idle":"2024-11-05T05:04:10.455658Z","shell.execute_reply.started":"2024-11-05T05:04:10.446988Z","shell.execute_reply":"2024-11-05T05:04:10.454060Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# create langchang RAG chain\nqa_chain = create_rag_chain(vectorstore)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T05:04:11.129856Z","iopub.execute_input":"2024-11-05T05:04:11.130293Z","iopub.status.idle":"2024-11-05T05:04:11.136914Z","shell.execute_reply.started":"2024-11-05T05:04:11.130254Z","shell.execute_reply":"2024-11-05T05:04:11.135486Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"question = \"심사위원을 누가 맡았어?\"\n\n# result = qa_chain({\"query\": question})\nresult = qa_chain.invoke({\"query\": question})\n\nprint (\"결과:\")\nprint(result[\"result\"])\n\nprint(\"출처:\")\nfor doc in result[\"source_documents\"]:\n    print(doc.page_content)\n    print(\"---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T05:04:11.506126Z","iopub.execute_input":"2024-11-05T05:04:11.506672Z","iopub.status.idle":"2024-11-05T05:04:11.894402Z","shell.execute_reply.started":"2024-11-05T05:04:11.506616Z","shell.execute_reply":"2024-11-05T05:04:11.893261Z"}},"outputs":[{"name":"stdout","text":"결과:\n문맥을 참고하여 질문에 정확하고 간결하게 답하십시오.\n    문맥: 《흑백요리사: 요리 계급 전쟁》(영어: Culinary Class Wars)은 넷플릭스의 요리 서바이벌 프로그램이다. 방송 직후 세계 여러 나라에서 시청률 1위를 기록했고, 대만인들의 한국 관광 열풍과 한국 음식에 대한 사랑을 불러일으켰다. 유명 레스토랑 셰프 등 100인의 요리사가 출연한다. 심사위원은 백종원과 안성재가 맡았다. 가제는 《호날두요리사》였다.[1]\n    질문: 심사위원을 누가 맡았어?\n    답변: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원: 쉐프들이 심사위원으로 참여했어?\n( 쉐프들이 심사위원으로 참여했어?)\n심사위원:\n출처:\n《흑백요리사: 요리 계급 전쟁》(영어: Culinary Class Wars)은 넷플릭스의 요리 서바이벌 프로그램이다. 방송 직후 세계 여러 나라에서 시청률 1위를 기록했고, 대만인들의 한국 관광 열풍과 한국 음식에 대한 사랑을 불러일으켰다. 유명 레스토랑 셰프 등 100인의 요리사가 출연한다. 심사위원은 백종원과 안성재가 맡았다. 가제는 《호날두요리사》였다.[1]\n---\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"docs = vectorstore.as_retriever().get_relevant_documents(question)\ndocs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T05:04:11.896431Z","iopub.execute_input":"2024-11-05T05:04:11.896845Z","iopub.status.idle":"2024-11-05T05:04:12.032752Z","shell.execute_reply.started":"2024-11-05T05:04:11.896805Z","shell.execute_reply":"2024-11-05T05:04:12.031632Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/2742417880.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n  docs = vectorstore.as_retriever().get_relevant_documents(question)\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"[Document(metadata={'source': 'https://ko.wikipedia.org/wiki/흑백요리사'}, page_content='《흑백요리사: 요리 계급 전쟁》(영어: Culinary Class Wars)은 넷플릭스의 요리 서바이벌 프로그램이다. 방송 직후 세계 여러 나라에서 시청률 1위를 기록했고, 대만인들의 한국 관광 열풍과 한국 음식에 대한 사랑을 불러일으켰다. 유명 레스토랑 셰프 등 100인의 요리사가 출연한다. 심사위원은 백종원과 안성재가 맡았다. 가제는 《호날두요리사》였다.[1]')]"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"docs = vectorstore.similarity_search(question, k=4)\ndocs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T05:04:12.841536Z","iopub.execute_input":"2024-11-05T05:04:12.841993Z","iopub.status.idle":"2024-11-05T05:04:12.915853Z","shell.execute_reply.started":"2024-11-05T05:04:12.841949Z","shell.execute_reply":"2024-11-05T05:04:12.914629Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"[Document(metadata={'source': 'https://ko.wikipedia.org/wiki/흑백요리사'}, page_content='《흑백요리사: 요리 계급 전쟁》(영어: Culinary Class Wars)은 넷플릭스의 요리 서바이벌 프로그램이다. 방송 직후 세계 여러 나라에서 시청률 1위를 기록했고, 대만인들의 한국 관광 열풍과 한국 음식에 대한 사랑을 불러일으켰다. 유명 레스토랑 셰프 등 100인의 요리사가 출연한다. 심사위원은 백종원과 안성재가 맡았다. 가제는 《호날두요리사》였다.[1]')]"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# It seems vectorDB loading from embedding model works fine, but seems llm model does not.\n# Some Korean llm model seems to work fine in text-generation task, but for Question-Ansering task, we might need another approach.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T05:04:13.218598Z","iopub.execute_input":"2024-11-05T05:04:13.219014Z","iopub.status.idle":"2024-11-05T05:04:13.223973Z","shell.execute_reply.started":"2024-11-05T05:04:13.218977Z","shell.execute_reply":"2024-11-05T05:04:13.222787Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Use QA pipeline with vectorstor similarity search","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\n# Load model and tokenizer\nmodel_name = \"yjgwak/klue-bert-base-finetuned-squard-kor-v1\"\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Set Q_A pipeline\nqa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T05:04:14.570150Z","iopub.execute_input":"2024-11-05T05:04:14.571531Z","iopub.status.idle":"2024-11-05T05:04:25.740357Z","shell.execute_reply.started":"2024-11-05T05:04:14.571480Z","shell.execute_reply":"2024-11-05T05:04:25.739128Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/635 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"967e9b44c8264f458161fe72b8a25452"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"495a4d1ac0254a87bcd1395f4c0867b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/367 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f6b0153858c4a6c867247bfecbfb7c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/246k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6526800b11447d78fff42b2cd0aa18a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb91e1678e054c719bd7d735ea3c674f"}},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"# Example: define question and context \nquestion = \"오늘 날씨 어때?\"\ncontext = \"오늘의 날씨는 맑고 따뜻한 기온이 유지될 것으로 보입니다.\"\n\n# model chain\nresult = qa_pipeline(question=question, context=context)\n\n# Result\nprint(\"질문:\", question)\nprint(\"답변:\", result['answer'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T05:04:25.742283Z","iopub.execute_input":"2024-11-05T05:04:25.742681Z","iopub.status.idle":"2024-11-05T05:04:25.856947Z","shell.execute_reply.started":"2024-11-05T05:04:25.742644Z","shell.execute_reply":"2024-11-05T05:04:25.855680Z"}},"outputs":[{"name":"stdout","text":"질문: 오늘 날씨 어때?\n답변: 맑고 따뜻한 기온이\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# search context in VectorStore\ndef retrieve_context(question, vectorstore):\n    docs = vectorstore.similarity_search(question, k=1)\n    if docs:\n        return docs[0].page_content  # return first relevant doc\n    else:\n        return None\n\n# Generate answer based on query and searched context similar to RAG chain\ndef answer_question_with_context(question, vectorstore):\n    context = retrieve_context(question, vectorstore)\n    if context:\n        result = qa_pipeline(question=question, context=context)\n        return result['answer'], context  # return answer and used source doc\n    else:\n        return \"관련 문맥을 찾지 못했습니다.\", None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T05:04:25.859058Z","iopub.execute_input":"2024-11-05T05:04:25.859629Z","iopub.status.idle":"2024-11-05T05:04:25.867519Z","shell.execute_reply.started":"2024-11-05T05:04:25.859571Z","shell.execute_reply":"2024-11-05T05:04:25.866268Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Example\nquestion = \"심사위원을 누가 맡았어?\"\n\nanswer, used_context = answer_question_with_context(question, vectorstore)\n\nprint(\"질문:\", question)\nprint(\"답변:\", answer)\nprint(\"사용된 문맥:\", used_context)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T05:04:25.870409Z","iopub.execute_input":"2024-11-05T05:04:25.871388Z","iopub.status.idle":"2024-11-05T05:04:26.190481Z","shell.execute_reply.started":"2024-11-05T05:04:25.871319Z","shell.execute_reply":"2024-11-05T05:04:26.189257Z"}},"outputs":[{"name":"stdout","text":"질문: 심사위원을 누가 맡았어?\n답변: 백종원과 안성재가\n사용된 문맥: 《흑백요리사: 요리 계급 전쟁》(영어: Culinary Class Wars)은 넷플릭스의 요리 서바이벌 프로그램이다. 방송 직후 세계 여러 나라에서 시청률 1위를 기록했고, 대만인들의 한국 관광 열풍과 한국 음식에 대한 사랑을 불러일으켰다. 유명 레스토랑 셰프 등 100인의 요리사가 출연한다. 심사위원은 백종원과 안성재가 맡았다. 가제는 《호날두요리사》였다.[1]\n","output_type":"stream"}],"execution_count":21}]}