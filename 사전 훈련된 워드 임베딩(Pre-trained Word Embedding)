{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install gensim","metadata":{"execution":{"iopub.status.busy":"2024-10-31T00:27:55.707446Z","iopub.execute_input":"2024-10-31T00:27:55.707868Z","iopub.status.idle":"2024-10-31T00:28:16.255557Z","shell.execute_reply.started":"2024-10-31T00:27:55.707827Z","shell.execute_reply":"2024-10-31T00:28:16.254251Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: gensim in /opt/conda/lib/python3.10/site-packages (4.3.3)\nRequirement already satisfied: numpy<2.0,>=1.18.5 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.26.4)\nCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n  Downloading scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim) (7.0.4)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\nDownloading scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: scipy\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.14.1\n    Uninstalling scipy-1.14.1:\n      Successfully uninstalled scipy-1.14.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.3 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntsfresh 0.20.3 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed scipy-1.13.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom collections import Counter\nimport gensim","metadata":{"execution":{"iopub.status.busy":"2024-10-31T00:28:26.694295Z","iopub.execute_input":"2024-10-31T00:28:26.694698Z","iopub.status.idle":"2024-10-31T00:28:35.085075Z","shell.execute_reply.started":"2024-10-31T00:28:26.694659Z","shell.execute_reply":"2024-10-31T00:28:35.084163Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# 문장의 긍, 부정을 판단하는 감성 분류 모델을 만들어봅시다. \n# 문장과 레이블 데이터를 만들었습니다. 긍정인 문장은 레이블 1, \n# 부정인 문장은 레이블이 0입니다.\n\nsentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', 'supreme quality', 'bad', 'highly respectable']\ny_train = [1, 0, 0, 1, 1, 0, 1]","metadata":{"execution":{"iopub.status.busy":"2024-10-31T00:28:57.838505Z","iopub.execute_input":"2024-10-31T00:28:57.839820Z","iopub.status.idle":"2024-10-31T00:28:57.844835Z","shell.execute_reply.started":"2024-10-31T00:28:57.839771Z","shell.execute_reply":"2024-10-31T00:28:57.843699Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# 각 샘플에 대해서 단어 토큰화를 수행\ntokenized_sentences = [sent.split() for sent in sentences]\nprint('단어 토큰화 된 결과 :', tokenized_sentences)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T00:29:09.582775Z","iopub.execute_input":"2024-10-31T00:29:09.583183Z","iopub.status.idle":"2024-10-31T00:29:09.589028Z","shell.execute_reply.started":"2024-10-31T00:29:09.583146Z","shell.execute_reply":"2024-10-31T00:29:09.587755Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"단어 토큰화 된 결과 : [['nice', 'great', 'best', 'amazing'], ['stop', 'lies'], ['pitiful', 'nerd'], ['excellent', 'work'], ['supreme', 'quality'], ['bad'], ['highly', 'respectable']]\n","output_type":"stream"}]},{"cell_type":"code","source":"# 토큰화 된 결과를 바탕으로 단어 집합을 만들어봅시다.\n# 우선 Counter() 모듈을 이용하여 각 단어의 등장 빈도수를 기록\nword_list = []\nfor sent in tokenized_sentences:\n    for word in sent:\n      word_list.append(word)\n\nword_counts = Counter(word_list)\nprint('총 단어수 :', len(word_counts))","metadata":{"execution":{"iopub.status.busy":"2024-10-31T00:29:48.360411Z","iopub.execute_input":"2024-10-31T00:29:48.360846Z","iopub.status.idle":"2024-10-31T00:29:48.367558Z","shell.execute_reply.started":"2024-10-31T00:29:48.360806Z","shell.execute_reply":"2024-10-31T00:29:48.366411Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"총 단어수 : 15\n","output_type":"stream"}]},{"cell_type":"code","source":"# 등장 빈도순으로 정렬\nvocab = sorted(word_counts, key=word_counts.get, reverse=True)\nprint(vocab)\n\n# nice가 등장 빈도수로 가장 높은 단어\n# 그 다음은 great, 그 다음은 best로 등장 빈도가 높은 순서대로 단어가 정렬된 상태","metadata":{"execution":{"iopub.status.busy":"2024-10-31T00:29:57.124703Z","iopub.execute_input":"2024-10-31T00:29:57.125717Z","iopub.status.idle":"2024-10-31T00:29:57.130755Z","shell.execute_reply.started":"2024-10-31T00:29:57.125643Z","shell.execute_reply":"2024-10-31T00:29:57.129664Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"['nice', 'great', 'best', 'amazing', 'stop', 'lies', 'pitiful', 'nerd', 'excellent', 'work', 'supreme', 'quality', 'bad', 'highly', 'respectable']\n","output_type":"stream"}]},{"cell_type":"code","source":"# 0번은 패딩 토큰을 위한 용도로 사용하고, \n# 1번은 단어 집합에 없는 단어가 등장하는 OOV(Out-Of-Vocabulary) 문제가 발생하면 \n# 사용하는 용도로 각각 할당\n\nword_to_index = {}\nword_to_index['<PAD>'] = 0\nword_to_index['<UNK>'] = 1\n\nfor index, word in enumerate(vocab) :\n  word_to_index[word] = index + 2\n\nvocab_size = len(word_to_index)\nprint('패딩 토큰, UNK 토큰을 고려한 단어 집합의 크기 :', vocab_size)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T00:31:08.227766Z","iopub.execute_input":"2024-10-31T00:31:08.228254Z","iopub.status.idle":"2024-10-31T00:31:08.235306Z","shell.execute_reply.started":"2024-10-31T00:31:08.228214Z","shell.execute_reply":"2024-10-31T00:31:08.234233Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"패딩 토큰, UNK 토큰을 고려한 단어 집합의 크기 : 17\n","output_type":"stream"}]},{"cell_type":"code","source":"# 단어 집합의 크기는 17입니다. 출력 결과\nprint(word_to_index)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T00:31:29.107163Z","iopub.execute_input":"2024-10-31T00:31:29.107660Z","iopub.status.idle":"2024-10-31T00:31:29.113697Z","shell.execute_reply.started":"2024-10-31T00:31:29.107614Z","shell.execute_reply":"2024-10-31T00:31:29.112343Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"{'<PAD>': 0, '<UNK>': 1, 'nice': 2, 'great': 3, 'best': 4, 'amazing': 5, 'stop': 6, 'lies': 7, 'pitiful': 8, 'nerd': 9, 'excellent': 10, 'work': 11, 'supreme': 12, 'quality': 13, 'bad': 14, 'highly': 15, 'respectable': 16}\n","output_type":"stream"}]},{"cell_type":"code","source":"# 단어 집합을 이용하여 정수 인코딩을 진행합니다. \n# 단어 집합에 없는 단어가 등장할 경우에는 정수 1이 할당되지만 \n# 이번 실습에서는 학습 데이터에 단어 집합에 없는 단어가 존재하지 않으므로 해당되지 않습니다.\n\ndef texts_to_sequences(tokenized_X_data, word_to_index):\n  encoded_X_data = []\n  for sent in tokenized_X_data:\n    index_sequences = []\n    for word in sent:\n      try:\n          index_sequences.append(word_to_index[word])\n      except KeyError:\n          index_sequences.append(word_to_index['<UNK>'])\n    encoded_X_data.append(index_sequences)\n  return encoded_X_data\n\nX_encoded = texts_to_sequences(tokenized_sentences, word_to_index)\nprint(X_encoded)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-31T00:32:00.347127Z","iopub.execute_input":"2024-10-31T00:32:00.347559Z","iopub.status.idle":"2024-10-31T00:32:00.355384Z","shell.execute_reply.started":"2024-10-31T00:32:00.347520Z","shell.execute_reply":"2024-10-31T00:32:00.354188Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"[[2, 3, 4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14], [15, 16]]\n","output_type":"stream"}]},{"cell_type":"code","source":"# 현재 데이터의 최대 길이를 측정하고, 해당 길이로 패딩을 진행\nmax_len = max(len(l) for l in X_encoded)\nprint('최대 길이 :',max_len)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T00:32:15.611434Z","iopub.execute_input":"2024-10-31T00:32:15.611841Z","iopub.status.idle":"2024-10-31T00:32:15.617508Z","shell.execute_reply.started":"2024-10-31T00:32:15.611803Z","shell.execute_reply":"2024-10-31T00:32:15.616408Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"최대 길이 : 4\n","output_type":"stream"}]},{"cell_type":"code","source":"def pad_sequences(sentences, max_len):\n  features = np.zeros((len(sentences), max_len), dtype=int)\n  for index, sentence in enumerate(sentences):\n    if len(sentence) != 0:\n      features[index, :len(sentence)] = np.array(sentence)[:max_len]\n  return features\n\nX_train = pad_sequences(X_encoded, max_len=max_len)\ny_train = np.array(y_train)\nprint('패딩 결과 :')\nprint(X_train)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T00:32:22.516028Z","iopub.execute_input":"2024-10-31T00:32:22.516948Z","iopub.status.idle":"2024-10-31T00:32:22.525216Z","shell.execute_reply.started":"2024-10-31T00:32:22.516902Z","shell.execute_reply":"2024-10-31T00:32:22.524089Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"패딩 결과 :\n[[ 2  3  4  5]\n [ 6  7  0  0]\n [ 8  9  0  0]\n [10 11  0  0]\n [12 13  0  0]\n [14  0  0  0]\n [15 16  0  0]]\n","output_type":"stream"}]},{"cell_type":"code","source":"# 모든 데이터의 길이가 4로 변환된 것을 확인하였습니다. \n# 이제 nn.Embedding()를 이용하여 모델을 설계\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader, TensorDataset","metadata":{"execution":{"iopub.status.busy":"2024-10-31T00:32:42.372319Z","iopub.execute_input":"2024-10-31T00:32:42.372814Z","iopub.status.idle":"2024-10-31T00:32:45.377244Z","shell.execute_reply.started":"2024-10-31T00:32:42.372771Z","shell.execute_reply":"2024-10-31T00:32:45.376226Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class SimpleModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim):\n        super(SimpleModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.flatten = nn.Flatten()\n        self.fc = nn.Linear(embedding_dim * max_len, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # embedded.shape == (배치 크기, 문장의 길이, 임베딩 벡터의 차원)\n        embedded = self.embedding(x)\n\n        # flattend.shape == (배치 크기, 문장의 길이 × 임베딩 벡터의 차원)\n        flattened = self.flatten(embedded)\n\n        # output.shape == (배치 크기, 1)\n        output = self.fc(flattened)\n        return self.sigmoid(output)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T00:32:51.369354Z","iopub.execute_input":"2024-10-31T00:32:51.370385Z","iopub.status.idle":"2024-10-31T00:32:51.377729Z","shell.execute_reply.started":"2024-10-31T00:32:51.370340Z","shell.execute_reply":"2024-10-31T00:32:51.376332Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# 모델 객체를 선언합니다. 임베딩 벡터의 크기는 100으로 정했습니다.\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nembedding_dim = 100\nsimple_model = SimpleModel(vocab_size, embedding_dim).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T00:33:05.931248Z","iopub.execute_input":"2024-10-31T00:33:05.931670Z","iopub.status.idle":"2024-10-31T00:33:05.977831Z","shell.execute_reply.started":"2024-10-31T00:33:05.931632Z","shell.execute_reply":"2024-10-31T00:33:05.976767Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# 로지스틱 회귀를 이용한 이진 분류 문제를 푸는 모델이므로 \n# 손실 함수로는 바이너리 크로스엔트로피 함수에 해당하는 nn.BCELoss()를 사용합니다.\n\ncriterion = nn.BCELoss()\noptimizer = Adam(simple_model.parameters())","metadata":{"execution":{"iopub.status.busy":"2024-10-31T00:33:31.358539Z","iopub.execute_input":"2024-10-31T00:33:31.359297Z","iopub.status.idle":"2024-10-31T00:33:32.455918Z","shell.execute_reply.started":"2024-10-31T00:33:31.359252Z","shell.execute_reply":"2024-10-31T00:33:32.454787Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# 데이터를 배치 크기 2로 설정한 데이터로더로 변환\ntrain_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.long), torch.tensor(y_train, dtype=torch.float32))\ntrain_dataloader = DataLoader(train_dataset, batch_size=2)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T00:33:42.581927Z","iopub.execute_input":"2024-10-31T00:33:42.582576Z","iopub.status.idle":"2024-10-31T00:33:42.606405Z","shell.execute_reply.started":"2024-10-31T00:33:42.582535Z","shell.execute_reply":"2024-10-31T00:33:42.605099Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# 데이터가 7개였으므로 배치 크기 2로 묶으면 총 묶음은 4개(2개, 2개, 2개, 1개)가 됩니다.\nprint(len(train_dataloader))","metadata":{"execution":{"iopub.status.busy":"2024-10-31T00:33:56.986750Z","iopub.execute_input":"2024-10-31T00:33:56.987211Z","iopub.status.idle":"2024-10-31T00:33:56.994867Z","shell.execute_reply.started":"2024-10-31T00:33:56.987169Z","shell.execute_reply":"2024-10-31T00:33:56.993487Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"4\n","output_type":"stream"}]},{"cell_type":"code","source":"# 총 10번 학습\nfor epoch in range(10):\n    for inputs, targets in train_dataloader:\n        # inputs.shape == (배치 크기, 문장 길이)\n        # targets.shape == (배치 크기)\n        inputs, targets = inputs.to(device), targets.to(device)\n\n        optimizer.zero_grad()\n\n        # outputs.shape == (배치 크기)\n        outputs = simple_model(inputs).view(-1) \n\n        loss = criterion(outputs, targets)\n        loss.backward()\n\n        optimizer.step()\n\n    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-31T00:34:08.176494Z","iopub.execute_input":"2024-10-31T00:34:08.176905Z","iopub.status.idle":"2024-10-31T00:34:08.325206Z","shell.execute_reply.started":"2024-10-31T00:34:08.176866Z","shell.execute_reply":"2024-10-31T00:34:08.323777Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 0.5656630992889404\nEpoch 2, Loss: 0.4259263575077057\nEpoch 3, Loss: 0.3162075877189636\nEpoch 4, Loss: 0.24398580193519592\nEpoch 5, Loss: 0.19914478063583374\nEpoch 6, Loss: 0.1711779236793518\nEpoch 7, Loss: 0.15253165364265442\nEpoch 8, Loss: 0.13838817179203033\nEpoch 9, Loss: 0.1260053962469101\nEpoch 10, Loss: 0.11416992545127869\n","output_type":"stream"}]},{"cell_type":"code","source":"# 2. 사전 훈련된 임베딩을 사용\n# 구글에서 사전 학습시킨 Word2Vec 모델을 사용하여 문제를 풀어봅시다.\n# 우선 구글에서 사전 학습시킨 Word2Vec 모델을 다운로드 합니다.\n\n!pip install gdown\n!gdown https://drive.google.com/uc?id=1Av37IVBQAAntSe1X3MOAl5gvowQzd2_j","metadata":{"execution":{"iopub.status.busy":"2024-10-31T00:34:43.212911Z","iopub.execute_input":"2024-10-31T00:34:43.213397Z","iopub.status.idle":"2024-10-31T00:35:31.197187Z","shell.execute_reply.started":"2024-10-31T00:34:43.213357Z","shell.execute_reply":"2024-10-31T00:35:31.195121Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Collecting gdown\n  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.15.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.4)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.8.30)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nDownloading gdown-5.2.0-py3-none-any.whl (18 kB)\nInstalling collected packages: gdown\nSuccessfully installed gdown-5.2.0\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1Av37IVBQAAntSe1X3MOAl5gvowQzd2_j\nFrom (redirected): https://drive.google.com/uc?id=1Av37IVBQAAntSe1X3MOAl5gvowQzd2_j&confirm=t&uuid=b66274f4-c7fc-4a82-8d62-47cd965cdd56\nTo: /kaggle/working/GoogleNews-vectors-negative300.bin.gz\n100%|██████████████████████████████████████| 1.65G/1.65G [00:30<00:00, 54.5MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# 구글의 사전 훈련된 Word2vec 모델을 로드합니다.\nword2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True) ","metadata":{"execution":{"iopub.status.busy":"2024-10-31T00:35:36.879913Z","iopub.execute_input":"2024-10-31T00:35:36.880401Z","iopub.status.idle":"2024-10-31T00:36:34.516951Z","shell.execute_reply.started":"2024-10-31T00:35:36.880355Z","shell.execute_reply":"2024-10-31T00:36:34.515882Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# 위 모델은 각 벡터가 300차원으로 구성되어져 있습니다. \n# 풀고자 하는 문제의 단어 집합 크기의 행과 300개의 열을 가지는 행렬 생성합니다. \n# 이 행렬의 값은 전부 0으로 채웁니다. 이 행렬에 사전 훈련된 임베딩 값을 넣어줄 것입니다.\n\nembedding_matrix = np.zeros((vocab_size, 300))\nprint('임베딩 행렬의 크기 :', embedding_matrix.shape)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T00:37:25.089706Z","iopub.execute_input":"2024-10-31T00:37:25.090236Z","iopub.status.idle":"2024-10-31T00:37:25.098767Z","shell.execute_reply.started":"2024-10-31T00:37:25.090193Z","shell.execute_reply":"2024-10-31T00:37:25.097039Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"임베딩 행렬의 크기 : (17, 300)\n","output_type":"stream"}]},{"cell_type":"code","source":"# word2vec_model에서 특정 단어를 입력하면 해당 단어의 임베딩 벡터를 리턴받을텐데, \n# 만약 word2vec_model에 특정 단어의 임베딩 벡터가 없다면 None을 리턴하도록 \n# 하는 함수 get_vector()를 구현\n\ndef get_vector(word):\n    if word in word2vec_model:\n        return word2vec_model[word]\n    else:\n        return None","metadata":{"execution":{"iopub.status.busy":"2024-10-31T00:37:56.168472Z","iopub.execute_input":"2024-10-31T00:37:56.169296Z","iopub.status.idle":"2024-10-31T00:37:56.174436Z","shell.execute_reply.started":"2024-10-31T00:37:56.169250Z","shell.execute_reply":"2024-10-31T00:37:56.173200Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# 단어 집합으로부터 단어를 1개씩 호출하여 word2vec_model에 해당 단어의 임베딩 벡터값이 \n# 존재하는지 확인합니다. 만약 None이 아니라면 존재한다는 의미이므로 임베딩 행렬에 \n# 해당 단어의 인덱스 위치의 행에 임베딩 벡터의 값을 저장\n\n# <PAD>를 위한 0번과 <UNK>를 위한 1번은 실제 단어가 아니므로 맵핑에서 제외\nfor word, i in word_to_index.items():\n    if i > 2:\n      temp = get_vector(word)\n      if temp is not None:\n          embedding_matrix[i] = temp","metadata":{"execution":{"iopub.status.busy":"2024-10-31T00:38:23.577396Z","iopub.execute_input":"2024-10-31T00:38:23.577920Z","iopub.status.idle":"2024-10-31T00:38:23.585252Z","shell.execute_reply.started":"2024-10-31T00:38:23.577868Z","shell.execute_reply":"2024-10-31T00:38:23.583816Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# 현재 풀고자하는 문제의 17개의 단어와 맵핑되는 임베딩 행렬이 완성됩니다. \n# 0번 단어는 패딩을 위한 용도이므로 사전 훈련된 임베딩 벡터값이 불필요합니다. \n# 이에 따라 초기값인 0벡터로 초기화가 되어져 있습니다. \n# embedding_matrix의 0번 위치의 벡터를 출력\n\n# <PAD>나 <UNK>의 경우는 사전 훈련된 임베딩이 들어가지 않아서 0벡터임\nprint(embedding_matrix[0])","metadata":{"execution":{"iopub.status.busy":"2024-10-31T00:38:51.473403Z","iopub.execute_input":"2024-10-31T00:38:51.473856Z","iopub.status.idle":"2024-10-31T00:38:51.527363Z","shell.execute_reply.started":"2024-10-31T00:38:51.473816Z","shell.execute_reply":"2024-10-31T00:38:51.526057Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"}]},{"cell_type":"code","source":"word_to_index['great']\n\n# 기존의 단어 집합에서 단어 'great'가 정수로 몇 번인지 확인\n# ==> 3번임.","metadata":{"execution":{"iopub.status.busy":"2024-10-31T00:39:00.136360Z","iopub.execute_input":"2024-10-31T00:39:00.137221Z","iopub.status.idle":"2024-10-31T00:39:00.145414Z","shell.execute_reply.started":"2024-10-31T00:39:00.137179Z","shell.execute_reply":"2024-10-31T00:39:00.144234Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"3"},"metadata":{}}]},{"cell_type":"code","source":"# 사전 훈련된 word2vec_model에서의 'great' 벡터와 \n# 현재 사전 훈련된 임베딩 벡터가 맵핑된 embedding_matrix의 3번 벡터가 동일한지 확인\n\n# word2vec_model에서 'great'의 임베딩 벡터\n# embedding_matrix[3]이 일치하는지 체크\nnp.all(word2vec_model['great'] == embedding_matrix[3])\n\n# 현재 3번 위치에 단어 'great' 벡터가 정상적으로 할당되었음을 의미","metadata":{"execution":{"iopub.status.busy":"2024-10-31T00:39:44.042026Z","iopub.execute_input":"2024-10-31T00:39:44.042440Z","iopub.status.idle":"2024-10-31T00:39:44.049782Z","shell.execute_reply.started":"2024-10-31T00:39:44.042402Z","shell.execute_reply":"2024-10-31T00:39:44.048619Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"class PretrainedEmbeddingModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim):\n        super(PretrainedEmbeddingModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = True\n        self.flatten = nn.Flatten()\n        self.fc = nn.Linear(embedding_dim * max_len, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        embedded = self.embedding(x)\n        flattened = self.flatten(embedded)\n        output = self.fc(flattened)\n        return self.sigmoid(output)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T00:40:12.042608Z","iopub.execute_input":"2024-10-31T00:40:12.043052Z","iopub.status.idle":"2024-10-31T00:40:12.050677Z","shell.execute_reply.started":"2024-10-31T00:40:12.042986Z","shell.execute_reply":"2024-10-31T00:40:12.049471Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# 모델 객체를 선언합니다. \n# 이때 임베딩 벡터의 크기는 embedding_matrix에서 \n# 이미 정해진 임베딩 벡터의 차원인 300으로 해야만 합니다.\n\npretraiend_embedding_model = PretrainedEmbeddingModel(vocab_size, 300).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T00:40:52.359137Z","iopub.execute_input":"2024-10-31T00:40:52.360330Z","iopub.status.idle":"2024-10-31T00:40:52.368641Z","shell.execute_reply.started":"2024-10-31T00:40:52.360281Z","shell.execute_reply":"2024-10-31T00:40:52.367462Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# 출력층에 로지스틱 회귀를 이용한 이진 분류 문제를 푸는 모델이므로 \n# 손실 함수로는 바이너리 크로스엔트로피 함수에 해당하는 nn.BCELoss()를 사용\n\ncriterion = nn.BCELoss()\noptimizer = Adam(pretraiend_embedding_model.parameters())","metadata":{"execution":{"iopub.status.busy":"2024-10-31T00:41:09.439449Z","iopub.execute_input":"2024-10-31T00:41:09.439899Z","iopub.status.idle":"2024-10-31T00:41:09.445384Z","shell.execute_reply.started":"2024-10-31T00:41:09.439859Z","shell.execute_reply":"2024-10-31T00:41:09.444094Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# 데이터가 7개였으므로 배치 크기 2로 묶으면 총 묶음은 4개(2개, 2개, 2개, 1개)가 됩니다.\nprint(len(train_dataloader))","metadata":{"execution":{"iopub.status.busy":"2024-10-31T00:41:20.574896Z","iopub.execute_input":"2024-10-31T00:41:20.575687Z","iopub.status.idle":"2024-10-31T00:41:20.580765Z","shell.execute_reply.started":"2024-10-31T00:41:20.575647Z","shell.execute_reply":"2024-10-31T00:41:20.579505Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"4\n","output_type":"stream"}]},{"cell_type":"code","source":"# 총 10번 학습\nfor epoch in range(10):\n    for inputs, targets in train_dataloader:\n        # inputs.shape == (배치 크기, 문장 길이)\n        # targets.shape == (배치 크기)\n        inputs, targets = inputs.to(device), targets.to(device)\n\n        optimizer.zero_grad()\n\n        # outputs.shape == (배치 크기)\n        outputs = pretraiend_embedding_model(inputs).view(-1) \n\n        loss = criterion(outputs, targets)\n        loss.backward()\n\n        optimizer.step()\n\n    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-31T00:41:36.000163Z","iopub.execute_input":"2024-10-31T00:41:36.000627Z","iopub.status.idle":"2024-10-31T00:41:36.054634Z","shell.execute_reply.started":"2024-10-31T00:41:36.000586Z","shell.execute_reply":"2024-10-31T00:41:36.053534Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 0.6813058257102966\nEpoch 2, Loss: 0.621146023273468\nEpoch 3, Loss: 0.5611085891723633\nEpoch 4, Loss: 0.5045236349105835\nEpoch 5, Loss: 0.4522863030433655\nEpoch 6, Loss: 0.40459784865379333\nEpoch 7, Loss: 0.3613838255405426\nEpoch 8, Loss: 0.3224439024925232\nEpoch 9, Loss: 0.28751519322395325\nEpoch 10, Loss: 0.2563055753707886\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}