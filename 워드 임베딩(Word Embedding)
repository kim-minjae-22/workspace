{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 희소 표현(Sparse Representation)\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-10-29T03:34:07.091890Z","iopub.execute_input":"2024-10-29T03:34:07.093116Z","iopub.status.idle":"2024-10-29T03:34:10.632840Z","shell.execute_reply.started":"2024-10-29T03:34:07.093011Z","shell.execute_reply":"2024-10-29T03:34:10.631614Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# 원-핫 벡터 생성\ndog = torch.FloatTensor([1, 0, 0, 0, 0])\ncat = torch.FloatTensor([0, 1, 0, 0, 0])\ncomputer = torch.FloatTensor([0, 0, 1, 0, 0])\nnetbook = torch.FloatTensor([0, 0, 0, 1, 0])\nbook = torch.FloatTensor([0, 0, 0, 0, 1])","metadata":{"execution":{"iopub.status.busy":"2024-10-29T03:34:17.604694Z","iopub.execute_input":"2024-10-29T03:34:17.605247Z","iopub.status.idle":"2024-10-29T03:34:17.626944Z","shell.execute_reply.started":"2024-10-29T03:34:17.605206Z","shell.execute_reply":"2024-10-29T03:34:17.625756Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"print(torch.cosine_similarity(dog, cat, dim=0))\nprint(torch.cosine_similarity(cat, computer, dim=0))\nprint(torch.cosine_similarity(computer, netbook, dim=0))\nprint(torch.cosine_similarity(netbook, book, dim=0))\n\n# 의미적 유사도를 반영할 수 없다는 것은 자연어 처리에서 치명적","metadata":{"execution":{"iopub.status.busy":"2024-10-29T03:34:25.949372Z","iopub.execute_input":"2024-10-29T03:34:25.949785Z","iopub.status.idle":"2024-10-29T03:34:26.058399Z","shell.execute_reply.started":"2024-10-29T03:34:25.949744Z","shell.execute_reply":"2024-10-29T03:34:26.057239Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"tensor(0.)\ntensor(0.)\ntensor(0.)\ntensor(0.)\n","output_type":"stream"}]},{"cell_type":"code","source":"# 밀집 표현(Dense Representation)\n# 밀집 표현은 벡터의 차원을 단어 집합의 크기로 상정하지 않습니다. \n# 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춥니다. \n\n# Ex) 강아지 = [ 0 0 0 0 1 0 0 0 0 0 0 0 ... 중략 ... 0] \n# 이 때 1 뒤의 0의 수는 9995개. 차원은 10,000\n\n# 0,000개의 단어가 있을 때 강아지란 단어를 표현하기 위해서는 위와 같은 표현을 사용했습니다. \n# 하지만 밀집 표현을 사용하고, 사용자가 밀집 표현의 차원을 128로 설정한다면, \n# 모든 단어의 벡터 표현의 차원은 128로 바뀌면서 모든 값이 실수가 됩니다.\n\n# Ex) 강아지 = [0.2 1.8 1.1 -2.1 1.1 2.8 ... 중략 ...] # 이 벡터의 차원은 128\n# 이 경우 벡터의 차원이 조밀해졌다고 하여 밀집 벡터(dense vector)라고 합니다.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 워드 임베딩(Word Embedding)\n# 단어를 밀집 벡터(dense vector)의 형태로 표현하는 방법을 워드 임베딩(word embedding)\n#  밀집 벡터를 워드 임베딩 과정을 통해 나온 결과라고 하여 임베딩 벡터(embedding vector)\n\n# 워드 임베딩 방법론으로는 LSA, Word2Vec, FastText, Glove 등","metadata":{},"execution_count":null,"outputs":[]}]}